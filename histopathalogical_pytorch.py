# -*- coding: utf-8 -*-
"""histopathalogical-pytorch

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JkNvC6mWGuF-YVo_xM--w2XxgWU8nUJb
"""

import os
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import dotenv
from PIL import Image
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from sklearn.model_selection import train_test_split

from ImageDataset import ImageDataset
from ConvNet96x96 import ConvNet96x96
from EarlyStopping import EarlyStopping


def download_data_from_kaggle():
    """
    Downloads the dataset from Kaggle and unzips it.
    Assumes environment variables KAGGLE_USERNAME and KAGGLE_KEY are set.
    """
    # Colab-specific: only use these if running on Google Colab
    try:
        from google.colab import userdata

        os.environ["KAGGLE_KEY"] = userdata.get("KAGGLE_KEY")
        os.environ["KAGGLE_USERNAME"] = userdata.get("KAGGLE_USERNAME")
    except ImportError:
        # load .env
        from dotenv import load_dotenv

        load_dotenv()

    # Download via Kaggle API
    os.system("kaggle competitions download -c histopathologic-cancer-detection")
    os.system("unzip -nq histopathologic-cancer-detection.zip")


if __name__ == "__main__":
    model_name = "histopathologic_cancer_detection"
    download_data_from_kaggle()
    train_dir = "train"
    test_dir = "test"
    csv_path = "train_labels.csv"

    # Load labels
    labels = pd.read_csv(csv_path)

    # Split into training and validation sets
    train_labels, val_labels = train_test_split(labels, test_size=0.2, random_state=42)

    # Define transformations for normalization, ResNet requires
    # resizing to at least 224 W and H
    transform = transforms.Compose(
        [
            transforms.Resize((96, 96)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomVerticalFlip(),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
        ]
    )

    predict_transform = transforms.Compose(
        [
            transforms.Resize((96, 96)),
            transforms.ToTensor(),
        ]
    )

    # Create datasets
    train_dataset = ImageDataset(train_labels, train_dir, transform=transform)
    val_dataset = ImageDataset(val_labels, train_dir, transform=predict_transform)

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)

    model = ConvNet96x96()

    # Move model to GPU if available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    print(model)

    # Binary Cross-Entropy Loss for binary classification
    criterion = nn.BCEWithLogitsLoss()

    # SGD optimizer
    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

    num_epochs = 100

    early_stopping = EarlyStopping(patience=5, delta=0.001)

    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")

        model.train()
        running_loss = 0.0

        for i, data in enumerate(train_loader, 0):
            print(f"Batch {i+1}/{len(train_loader)}", end="\r")
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs.squeeze(), labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        print(f"Training Loss: {running_loss / len(train_loader)}")
        print("Validation:")
        model.eval()
        val_loss = 0.0
        correct = 0

        with torch.no_grad():
            for data in val_loader:
                inputs, labels = data
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                val_loss += criterion(outputs.squeeze(), labels).item()
                predicted = (outputs.squeeze() > 0.5).long()
                correct += (predicted == labels).sum().item()
                # print(f"Validation Loss: {val_loss / len(val_loader)}")
        print(f"Validation Loss: {val_loss / len(val_loader)}")
        print(f"Validation Accuracy: {correct / len(val_dataset)}")

        # Early stopping logic
        early_stopping(val_loss, model)

        if early_stopping.early_stop:
            print("Early stopping triggered")
            break

    # Load the best model weights
    model.load_state_dict(early_stopping.get_best_weights())

    torch.save(model.state_dict(), model_name + ".pth")

    model.load_state_dict(torch.load(model_name + ".pth"))
    model.eval()

    def predict_image(image_path, model, transform, device):
        image = Image.open(image_path).convert("RGB")
        image = transform(image).unsqueeze(0).to(device)
        output = model(image).item()
        return "1" if output > 0.5 else "0"

    # for each image in test dir, predict and produce CSV with results
    import os
    import pandas as pd
    from PIL import Image

    results = []

    for filename in os.listdir(test_dir):
        if filename.endswith(".tif"):
            image_path = os.path.join(test_dir, filename)
            prediction = predict_image(image_path, model, predict_transform, device)
            id_no_ext = os.path.splitext(filename)[0]
            results.append({"id": id_no_ext, "label": prediction})

    results = pd.DataFrame(results)

    results.to_csv(model_name + ".csv", index=False)

    # !kaggle competitions submit -c histopathologic-cancer-detection -f {model_name + '.csv'} -m {model_name}
